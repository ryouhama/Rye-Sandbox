from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts.chat import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)


def run():
    llm = ChatOpenAI(temperature=0)
    memory = ConversationBufferMemory(return_messages=True)

    template = "ä»¥ä¸‹ã¯ã€äººé–“ã¨AIã®å‹å¥½çš„ãªä¼šè©±ã§ã™ã€‚AIã¯äººé–“ã®ç™ºè¨€ã‚’ç†è§£ã—ã€ãã‚Œã«å¯¾ã—ã¦é©åˆ‡ãªè¿”ç­”ã‚’ã—ã¾ã™ã€‚"

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”¨ã„ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(template),
        MessagesPlaceholder(variable_name="history"),
        HumanMessagePromptTemplate.from_template("{input}")
    ])
    conversation = ConversationChain(llm=llm, memory=memory, prompt=prompt)

    command = input("ğŸ˜‡ : ")

    while True:
        response = conversation.predict(input=command)
        print(f"ğŸ¤– : {response}")
        command = input("ğŸ˜‡ : ")
        if command == "exit":
            break


if __name__ == "__main__":
    load_dotenv()
    run()
